# GPT OSS Learning

ローカル環境でGPT OSSモデルを実行するための学習プロジェクトです。

Docker ComposeとOllamaを使用して、コマンドラインからカスタムプロンプトでLLMと対話できる環境を構築しています。

## 機能

- 🤖 Phi4-miniモデルによるローカルLLM実行
- 💬 カスタムプロンプト対応
- 🐳 Docker環境による分離実行
- 🔄 自動モデルダウンロード
- ⚡ ワンコマンドでの起動

## 前提条件

- Docker Desktop
- 4GB以上のメモリ
- 3GB以上の空きディスクスペース

## インストール

1. リポジトリをクローン
```bash
git clone <repository-url>
cd gpt-oss-learning
```

2. 起動スクリプトに実行権限を付与
```bash
chmod +x start.sh
```

## 使用方法

### カスタムプロンプトで実行
```bash
./start.sh "Pythonでソートアルゴリズムを説明して"
```

### デフォルトプロンプトで実行
```bash
./start.sh
```

### 実行例
```bash
# 技術的な質問
./start.sh "機械学習とは何ですか？初心者にもわかりやすく説明してください"

# 創作依頼
./start.sh "短い詩を作ってください"

# コーディング支援
./start.sh "Python でフィボナッチ数列を生成するコードを書いて"
```

## システム構成

```
┌─────────────────┐    ┌─────────────────┐
│   start.sh      │────│   main.py       │
│  (起動スクリプト)  │    │  (Python実行部)   │
└─────────────────┘    └─────────────────┘
           │                      │
           └──────────┬───────────┘
                      │
        ┌─────────────▼─────────────┐
        │      Docker Compose      │
        │  ┌─────────┐ ┌─────────┐  │
        │  │ Ollama  │ │   App   │  │
        │  │Container│ │Container│  │
        │  └─────────┘ └─────────┘  │
        └───────────────────────────┘
```

### 各コンポーネント

- **start.sh**: ユーザーインターフェース、環境起動制御
- **main.py**: LLMとの通信、レスポンス処理
- **Ollama Container**: Phi4-miniモデルのホスティング
- **App Container**: Python実行環境

## ファイル構成

```
.
├── README.md                    # このファイル
├── start.sh                     # 起動スクリプト
├── main.py                      # メインプログラム
├── docker-compose.yml           # Docker環境定義
├── Dockerfile                   # Pythonコンテナ定義
└── requirements.txt             # Python依存関係
```

## 技術スタック

- **LLM**: Phi4-mini (Microsoft)
- **実行基盤**: Ollama
- **コンテナ**: Docker Compose
- **プログラミング言語**: Python 3.11
- **ライブラリ**: OpenAI Python SDK, Requests

## トラブルシューティング

### メモリ不足エラー
Docker Desktopのメモリ設定を4GB以上に設定してください。

### モデルダウンロードが遅い
初回実行時は約2.5GBのモデルダウンロードが発生します。ネットワーク環境により時間がかかる場合があります。

### ポート競合エラー
ポート11434が使用中の場合、他のOllamaプロセスを停止してください：
```bash
docker stop ollama
```

## 開発ログ

このプロジェクトの詳細な開発過程は以下の記事で記録しています：

- [#1 LLMの仕組みを3つのレイヤーで整理してみた]
- [#2 ローカルマシンの容量チェックとDocker環境の準備]
- [#3 Phi4-miniの実行環境構築とカスタムプロンプト対応](./【学習ログ】ローカルでGPT%20OSSを動かす準備%20%233%20Phi4-miniの実行環境構築とカスタムプロンプト対応.md)

## ライセンス

このプロジェクトはMITライセンスの下で公開されています。

## 貢献

プルリクエストやイシューの報告を歓迎します。